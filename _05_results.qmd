# Results

## Exploratory Data Analysis

To test our intuition that Marvel’s newer content hasn’t been hitting the same highs as its earlier phases, we turned to the data. The figure above visualizes Rotten Tomatoes audience scores for Marvel media over time, from 2008 through 2025.

The trend is hard to miss: while early titles (roughly 2008–2018) generally received consistently high ratings—frequently in the 80–90% range—recent releases show a dramatic increase in variability. Starting around 2019, the scores become much more volatile, with several sharp dips into the 40s and 50s and far fewer guaranteed crowd-pleasers. Even well-established franchises aren't immune to this drop in reception.

This backs up what many longtime fans (ourselves included) have been feeling: the MCU just isn’t as consistent as it used to be. Audiences seem increasingly divided over the newer content, and not every experiment has paid off. At the same time, there's been a surge in the volume of media released. With more shows and movies hitting screens each year, the range of quality—and audience response—has grown.

This plot served as a useful gut-check: it confirmed that our hunch wasn’t just nostalgia talking. Something has changed in how Marvel stories are being received, and this volatility in reception helped motivate our deeper dive into the storytelling DNA of the comics themselves. If some source material leads to reliably well-received adaptations, and others don’t, then maybe the data can help us figure out the difference.

(I almost would include this graphic up front when you first bring this up. You don't have to give all the details around the data for it yet at that point, but it underpins much of what you are doing)

(I like you checking this, though I think also looking at the critic scores might be interesting. I'd definitely phrase it around an increase in variability, not necessarily a decrease in quality though, because the highs still seem just as high if not higher)

```{r echo = FALSE, message = FALSE}
#| fig-cap: "Figure X: A time series graph showing the Rotten Tomatoes Audience Scores for Marvel Media over Time."
options(repos = c(CRAN = "https://cran.rstudio.com/"))
library(tidyverse)
marvel_ratings<-read_csv("marvel_ratings.csv")
ggplot(marvel_ratings, aes(x=release_date, y = audience_score))+
  geom_line()+
  theme_minimal()+
  labs(title = "We're Not Crazy: Marvel's Just Not the Same",
       subtitle = "Marvel Media's Audience Score Over Time",
       x = "Release Date of Media",
       y = "Rotten Tomatoes Audience Score (Percent)",
       caption = "Data Source: Rotten Tomatoes")
```




## Model Building

(Is the composite rating an average of the two (critic and audience)? Or how exactly did you do that? Keep in mind that good research is reproducible, and that means you need to give a reader enough details that they could largely reproduce things on their own. Also, how many movies/shows did you have in each bucket after deciding on this threshold?)

(We talked about this after class last week, but don't be afraid to get into the weeds on this even if the end result didn't pan out. The goal is to show that you know what you are talking about and doing, and you should be able to do that even in cases where the model didn't end up working as hoped.)
Once we assembled our dataset, the next step was to determine which elements of comic storytelling actually correlate with a successful adaptation. To do this, we treated the task as a binary classification problem: given a comic issue’s metadata, could we predict whether it would be a strong candidate for MCU adaptation?

We began by defining a success label using the success_score variable—a composite rating derived from Rotten Tomatoes critic and audience scores. Comics tied to MCU content with a score of 85 or higher were labeled 1 (successful); all others were labeled 0. We intentionally set a high bar here: the goal was not to capture everything that was merely “good,” but to flag only the standout stories.

Before training, we handled missing data conservatively. Boolean indicators such as romance, mentor, and movie_based were treated as FALSE by default; numeric features like num_themes, num_characters, and num_power_types were filled in as 0. This approach helped preserve the structure of the data while minimizing the risk of overestimating character complexity or narrative richness.

### Round 1: Random Forests
We initially trained a Random Forest classifier using the caret and randomForest packages in R. The model showed striking performance during training—an AUC close to 1.0 and high overall accuracy. But the deeper we looked, the more issues we found.

Specifically, the model appeared to overfit the training data, likely memorizing correlated artifacts—like series identifiers—instead of learning generalizable narrative signals. One major source of leakage was the series_id column: if every issue in a given series shared the same success label, the model could trivially “learn” to predict success without using any true narrative signals.

Even after removing series_id, we applied multiple countermeasures, including class balancing via downsampling, 5-fold cross-validation, and feature selection to eliminate redundant predictors. Despite these efforts, the model continued to overfit. It performed exceptionally well on the dominant class (successful comics) but struggled to identify false positives and failed successes, resulting in poor generalization.

A quick look at the confusion matrix and ROC curve confirmed our suspicion: the model was accurate for the wrong reasons.

We also tested whether tree-based models could be salvaged through aggressive pruning. We trained a decision tree with a maximum depth of 3 and strict split constraints. However, the resulting model completely failed to recognize any successful comics, defaulting to the majority class and achieving a Kappa score of 0. Despite its high accuracy (87.1%), it had 0% sensitivity and a balanced accuracy of 0.5—effectively guessing at random. This confirmed that the limitations of tree-based models were not solely due to overfitting, but also a mismatch between model structure and task complexity. Logistic regression, in contrast, captured meaningful narrative patterns without relying on structural shortcuts.

--- edit this: Round 3: Constrained XGBoost
To better isolate whether overfitting was to blame, we trained a deliberately constrained XGBoost model, limiting tree depth, boosting rounds, and requiring large gains for splits. These constraints simulate a form of regularized learning, which reduces the model’s capacity to memorize spurious patterns.

The results were striking. With only shallow trees (max_depth = 2), a high minimum child weight, and gamma regularization, the model achieved:

Accuracy: 89.9%

AUC: 0.968

Kappa: 0.647

Sensitivity (true positive rate): 89.4%

Specificity (true negative rate): 92.9%

These metrics strongly suggest that Random Forests and default XGBoost were not inherently unsuitable, but rather insufficiently disciplined in how they handled noisy or unbalanced training data. Once regularization was applied, XGBoost delivered excellent performance without falling into the traps of earlier models.

That said, even with these constraints, interpretability remained a concern. Logistic regression provided more transparent feature weights and fewer failure modes when faced with edge cases or sparsely represented themes. So while constrained XGBoost could be a strong deployment model, logistic regression remained the better choice for exploration and explanation.

### Round 2: XGBoost
Hoping to find a better balance, we turned to XGBoost, a more sophisticated gradient boosting method. We trained the model on the same set of predictors, with class balancing and regularization in place.

While XGBoost slightly improved recall for the minority class (success_label == 0) under upsampling, the class imbalance remained a significant challenge. While precision remained high, recall for the unsuccessful class stayed low, and interpretability took a hit due to the model’s complexity. 

ADD MORE HERE

### Round 3: Logistic Regression

Ultimately, we landed on a logistic regression model. Before fitting the model, we ensured that binary factors were encoded as pos/neg and applied 5-fold cross-validation with upsampling to counteract the imbalance in success labels. While it didn’t dazzle with perfect accuracy, it offered several clear advantages:

Better class balance: It recognized both successful and unsuccessful comics more fairly.

More interpretable coefficients: We could identify which features actually mattered (e.g., num_themes, mentor, origin_story).

Fewer red flags: No sign of memorization or artifact-based prediction.

In terms of metrics, the final model reached:

- Accuracy: 81.2%

- Precision: 96.5%

- Recall: 71.0%

- AUC: 0.898

- Kappa: 0.425

The ROC curve looked promising—stable, clean, and free of the extreme curvature that often signals overfitting and the Kappa statistic (κ = 0.425) indicates moderate agreement between predicted and actual labels after accounting for chance agreement—an important validation that the model wasn’t simply riding the dominant class (as earlier models had done). While not perfect, this level of agreement reinforces that the model captures meaningful patterns in the data without overfitting.

## The Output
Although tree-based models often dominate performance benchmarks, in this case, they proved too powerful for their own good. Random Forest and XGBoost were easily distracted by artifacts in the data and susceptible to subtle forms of leakage. Logistic regression, by contrast, struck the right balance between predictive power and explanatory value.

More importantly, it helped us isolate actual narrative signals—traits that future adaptations could watch for, rather than shortcuts tied to past decisions. With this model in hand, we’re finally able to ask the question that started this whole project: what makes a comic story truly worth adapting?

