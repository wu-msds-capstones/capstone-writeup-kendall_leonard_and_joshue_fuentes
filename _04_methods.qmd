# Methods
Our methodological approach combines web scraping, data engineering, and machine learning. We gathered structured data from official and semi-official sources, created a relational schema to support analysis, and built models to identify which comic features correlate with successful MCU adaptations.

## Data Collection
Our project relied on building a unified dataset from our three sources—Rotten Tomatoes, Marvel.com, and IMDb—each with its own challenges and extraction strategies. Because no single source contained all of the information we needed in a clean, structured format, we combined multiple data acquisition methods, including custom Python-based web scrapers, point-and-click scraping tools, and selective use of existing datasets. In each case, we evaluated whether available APIs were usable; while Rotten Tomatoes provided no public API, Marvel’s certified API was tested but ultimately discarded due to severe data quality issues such as truncated text, incorrect field assignments, and missing metadata. As a result, we opted for targeted manual scraping approaches tailored to each platform’s structure, coupled with custom data cleaning, pattern matching, and keyword classification routines to transform raw web data into analysis-ready formats.


### Cinematic Ratings (Rotten Tomatoes):
We developed a scraper using Python’s BeautifulSoup and requests libraries to extract critic and audience scores from Rotten Tomatoes. The tool processed a curated list of MCU movies and shows, formatting each title for URL compatibility and accounting for exceptions such as punctuation or naming variations (e.g., Spider-Man: Homecoming or The Avengers). The scraper navigated to each title’s page and extracted three key metrics: the Tomatometer (critic score) and the Popcornmeter (audience score). These were stored in a structured CSV with columns for title, type (movie or TV show), release year, critic score, and audience score.

### Comic Metadata (Marvel.com):
To retrieve information about comic issues, we began by scraping the complete list of comic series available on Marvel.com. From there, we navigated into each series archive to collect issue-level data, including title, release year, description, and appearing characters. While Marvel does offer a certified API that theoretically provides this information, our early tests revealed significant quality problems—such as descriptions appearing in the title field, truncated text, incorrect issue numbers, and mismatched release dates. Because of these inconsistencies, we opted to build a custom scraper rather than rely on the API.


Scraping the issue pages introduced its own challenges, as the HTML structure and labeling varied widely—not just across series, but sometimes from issue to issue. This variability required the scraper to adapt to different page layouts and naming conventions to ensure complete capture of available metadata. The resulting raw fields were stored exactly as they appeared on the site, preserving the unaltered source data for downstream processing.

### Character Metadata (Marvel.com and Kaggle):
We initially relied on a Kaggle dataset provided by Professor Jed Rembold, which contained basic character information such as name, powers, and gender. However, the dataset included only about 750 characters—and not all of them were Marvel. After filtering out non-Marvel entries, we were left with just 387 characters, far short of the thousands that exist in the Marvel universe. This limited coverage meant that many characters present in our comics data had no corresponding entry in the Kaggle set, making it unsuitable as our primary source.


To address this, we shifted to scraping the Marvel.com character archive directly, due in part to similar API issues we had encountered with the comics data. While Marvel’s certified API exists, it suffered from inconsistent and incomplete data—such as missing fields, truncated descriptions, and occasional mismatches (e.g., a character’s civilian name being replaced with a location or incorrect alignment). Moreover, the API did not offer as much metadata as we needed for our analysis, omitting certain alias information, detailed power descriptions, and complete group affiliation histories.


Our scraper navigated the paginated Marvel.com character index and visited each individual character page to extract key fields: civilian name, aliases, detailed power descriptions, and group affiliations. The information was stored exactly as it appeared on the site, preserving the unaltered source text for downstream processing.

### IMDb Adaptation Links:
To determine which comic characters had appeared in the MCU, we scraped complete cast lists from IMDb pages for every MCU movie and television show. Unlike Marvel.com—which has an inconsistent HTML structure and labeling—IMDb uses a standardized page format, making it possible to extract data more efficiently. For this task, we used Webscraper.io, a point-and-click scraping tool that allowed us to define a structured site map for pulling all actor and character names without having to write a custom parser from scratch.


The tool navigated through each production’s cast list, collecting every credited character name. The extracted names were stored exactly as they appeared on IMDb, preserving the original formatting for later processing.

## Feature Engineering
Once the raw data was collected, we undertook a series of transformations and enrichment steps to make it analysis-ready. Across all sources, we performed both targeted data cleaning and structured feature creation, ensuring that raw scraped fields were preserved while generating new variables for modeling.

### Rotten Tomatoes
For the cinematic ratings data from Rotten Tomatoes, title strings were normalized to ensure URL compatibility, and known exceptions such as punctuation or naming variations were handled to maintain consistency across sources.

### Comic Meta Data
We developed keyword- and regular-expression-based routines to detect boolean features such as team-ups, significant events, recurring themes, and threat levels. For example, detecting a team-up with the X-Men required matching multiple phrasings, including “joins forces with,” “guest-starring,” or “X-Men appearance.” Similar approaches were applied to identify common themes (e.g., “time travel,” “political intrigue”), threat categories (e.g., “cosmic entity,” “street-level”), and major Marvel events (e.g., Civil War, Infinity Gauntlet).


Threat level classification was initially approached entirely through regex-based keyword matching. We curated separate lists of phrases and terms that we believed could indicate different categories of threat, including local criminal threats (e.g., “gang leader,” “organized crime”), city-level threats (e.g., “terrorist attack,” “mass destruction”), national or global threats (e.g., “world domination,” “international crisis”), and cosmic or interdimensional threats (e.g., “galactic invasion,” “alien armada,” “universal annihilation”). While this worked well in clear-cut cases—such as spotting the term “Galactus” to flag a cosmic-level threat—it quickly became clear that the diversity of phrasing in Marvel’s descriptions made consistent classification difficult.


Recognizing these limitations, we shifted to a semi-automated “threat helper” approach. We built a Python tool that combined our curated threat-level keyword lists with an AI-assisted workflow. The script ingested each comic’s description, scanned for potential threat indicators from our curated lists, and then generated a suggested classification into one of our predefined categories. These suggestions were presented for human review, allowing us to quickly confirm correct classifications or make adjustments when the AI’s suggestion missed context or nuance. This human-in-the-loop process dramatically expanded our coverage, increasing classified comics from roughly 500 under the pure-regex approach to over 11,000. While this still fell short of full coverage, it represented a substantial improvement.

### Character Meta Data
Like the comics data, character metadata from Marvel.com was inconsistently formatted, with powers sometimes listed as a single sentence (“Can control shadows”) or as a bulleted list, and group affiliations sometimes buried in biography text rather than in labeled sections.


To standardize this, we developed pattern-matching routines and keyword detection methods:


Powers: Matched terms from a curated power dictionary (e.g., “telepathy,” “superhuman strength”) across both list and free-text descriptions.


Group affiliations: Detected known team names (e.g., “Avengers,” “X-Men,” “Guardians of the Galaxy”) regardless of case or phrasing, including variations like “member of the Avengers” or “joined the Avengers.”


This process yielded 1,868 unique Marvel characters—nearly five times the coverage of the filtered Kaggle dataset—each assigned a unique ID and stored in the characters table. Both the raw scraped fields (exact text from Marvel.com) and our standardized, structured fields (boolean indicators for powers, team memberships, etc.) were preserved, ensuring we could analyze both the original descriptions and the engineered traits in downstream modeling.


We also experimented with regex-based extraction for additional character-level metadata, including nationality, sex, and other demographic attributes. For instance, we attempted to identify nationality by scanning for explicit mentions like “born in Wakanda,” “Canadian mutant,” or “Russian spy” within character biographies, and to determine sex by detecting gendered pronouns or descriptors such as “he/him,” “she/her,” “female hero,” or “male villain.” However, these approaches proved unreliable and were discarded to avoid bias.


### IMDb Data
While the consistent IMDb structure made scraping straightforward, the extracted data required cleaning before use. Characters with dual identities or aliases were often listed as a single string separated by a slash (e.g., “Steve Rogers/Captain America”). The scraping process sometimes splits these into separate cells, treating them as two distinct entries.


To address this, we post-processed the data by joining the split cells back into a single string before attempting to match them against our master characters table. Once this cleaning was complete, we performed name matching between the IMDb-scraped character list and our Marvel.com-sourced character metadata. Any match was assigned a binary in_movie flag, which became a crucial linking feature for our analysis—allowing us to tie specific comic traits (e.g., popularity, powers, story arc depth) to real-world MCU adaptation outcomes.


### Final Standardization
Throughout all of this, we also performed data cleaning and imputation to standardize the dataset. Boolean fields such as romance, mentor, and cultural significance were imputed as FALSE when missing, and numeric count variables—including number of themes, characters, and power types—were imputed as zero. This conservative imputation ensured that missing values did not artificially inflate the narrative richness or complexity of a comic. Finally, both the raw scraped text and the engineered boolean and numeric indicators were stored in our database, preserving the original source material while enabling structured, reproducible analysis for modeling.


## Methods for Analysis
Our analysis framed the problem as a binary classification task: given a comic issue’s metadata, could we predict whether it would be a strong candidate for a successful MCU adaptation? We first defined a success metric for adapted comics by averaging each MCU movie or series’s Rotten Tomatoes critic and audience scores—both converted to a 0–100 scale—to produce a composite “success score.” Issues with a score of 85 or higher were labeled as successful (1), while those below this threshold were labeled as unsuccessful (0). This threshold was chosen to highlight standout titles rather than merely above-average ones. Adaptations missing either critic or audience scores were excluded to avoid introducing bias from incomplete data.


Because the dataset was highly imbalanced—most comics have never been adapted, and even fewer qualified as successful adaptations—we implemented class-balancing techniques tailored to each modeling approach. For Random Forest, we applied downsampling of the majority class, while for logistic regression, we used upsampling of the minority class to ensure sufficient positive examples during training.


Feature engineering played a central role in preparing the dataset for modeling. Boolean variables such as romance, mentor, and cultural significance were imputed as FALSE when missing, while numeric count features like the number of themes, characters, and power types were imputed as zero. In addition to raw metadata, engineered features—such as thematic classifications, threat levels, and team affiliations—were incorporated to better capture narrative and structural characteristics.


We evaluated three modeling strategies, each chosen for a specific reason. First, we trained a Random Forest classifier using the caret package with 5-fold cross-validation. Random Forest was selected as our starting point because it is a strong, general-purpose baseline for tabular data, capable of capturing non-linear relationships and variable interactions without requiring extensive feature scaling. However, while it initially achieved strong performance on the training set, it suffered from overfitting caused in part by leakage from series-level identifiers. Removing these high-leakage features reduced but did not eliminate the problem, and recall for the minority class remained low.


Next, we implemented a constrained XGBoost model, chosen for its ability to model complex non-linear interactions while offering fine-grained control over regularization to reduce overfitting. Our goal was to see whether a carefully restricted tree-based model could retain predictive strength while avoiding the pitfalls seen in Random Forest. Hyperparameters were set to: max_depth = 2, nrounds = 50, eta = 0.3, gamma = 5, min_child_weight = 10, subsample = 0.7, and colsample_bytree = 0.5. These settings enforced shallow trees, strong regularization, and feature subsampling, which together encouraged generalization and reduced reliance on dataset quirks.


Finally, we trained a logistic regression model, chosen specifically for its interpretability. While tree-based methods can be opaque, logistic regression’s coefficients directly show the direction and magnitude of each feature’s influence on the outcome, making it ideal for explaining why certain comics ranked highly. All binary predictors were encoded as "pos"/"neg" to ensure consistent treatment, and the model was trained with 5-fold cross-validation using upsampling to balance the classes.


Because our objectives included both high predictive accuracy and interpretability, we combined the outputs of the constrained XGBoost and logistic regression models into an equal-weight ensemble. For each comic, we averaged the two models’ predicted probabilities of success to create a composite “success potential score.” This ensemble leveraged the non-linear interaction modeling strengths of XGBoost alongside the transparency of logistic regression, producing a ranked list of candidate comics with both a strong empirical basis and an interpretable rationale.


# Ethical Considerations
In constructing our dataset, we prioritized sourcing information from official or semi-official platforms—Rotten Tomatoes, Marvel.com, and IMDb—to ensure accuracy, attribution, and compliance with public data access guidelines. We explicitly avoided fan-maintained wikis, Reddit threads, or other user-generated content due to concerns over data reliability, consent, and attribution.


There was one intentional exception to this rule. The MCU does not maintain an official, comprehensive list of which comics or series contributed to each adaptation, and in many cases, a film draws from multiple sources. To fill this gap, we consulted aggregated fan consensus from reputable online discussions to determine which series had been adapted. These judgments were made only when there was broad agreement across sources and were recorded at the series level (marking the entire series as adapted or not) rather than attempting to classify individual issues. This series-level adaptation indicator served as an auxiliary reference for linking comics to MCU productions.


Bias in source material was also a central concern. Marvel’s official descriptions often reflect the creative and cultural context in which they were written, which can lead to underrepresentation or stereotyping. This has practical implications: our early regex-based attempts to classify characters by gender or nationality failed in part because Marvel’s text frequently omitted, obscured, or inconsistently presented these details—especially for alien, robotic, or alternate-universe characters. To avoid introducing systematic errors or reinforcing stereotypes, we chose to discard these automated classifications.


Bias in performance metrics must also be acknowledged. Rotten Tomatoes scores, while useful for comparing adaptations, are shaped by the composition and behavior of the voting population. Online review bombing, demographic skews, and differences in critic vs. audience priorities mean these scores are not purely objective measures of “success,” but rather culturally mediated proxies.


We also applied content filtering before feature engineering, excluding comics with problematic or outdated material that could bias analysis or introduce inappropriate themes into the dataset.


Privacy, data ownership, and responsible AI were important in our workflow. Although the AI-assisted “threat helper” only analyzed fictional descriptions, we treated its outputs as suggestions requiring human verification to prevent over-reliance on automated classification. This human-in-the-loop approach reduced the risk of amplifying AI misclassifications and maintained accountability for the final dataset. Additionally, all scraping adhered to public access guidelines, and we avoided proprietary, unpublished, or paywalled content.


Finally, we defined a clear scope for what qualified as an MCU adaptation: only the 48 feature films and television series officially produced under the MCU banner. While some characters also appear in non-MCU productions (e.g., the X-Men films), these were excluded from adaptation matching, as they were created outside MCU continuity. As a result, our final dataset included 27,520 comics, 1,868 unique Marvel characters, and adaptation metadata for the complete set of MCU productions to date. 


Finally, we recognize that predictive models like ours have real-world influence if used in industry settings. If deployed without context, a “success potential score” could unintentionally reinforce historical preferences in which stories are adapted, limiting diversity in both characters and narrative structures. We therefore frame our models as decision-support tools, intended to complement rather than replace human judgment, with explicit attention to representation, cultural sensitivity, and creative integrity.
